{
 "metadata": {
  "name": "ideas"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {},
     "source": [
      "Word Embedding Extension to (Chambers & Jurafsky, 2008)"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Let $\\mathcal{D}$ be a set of documents.\n",
      "\n",
      "Let $\\mathcal{C}$ be a set of verb pairs $v_1,v_2$ where $(v_1,v_2)\\in \\mathcal{C}$ i.f.f. $v_1,v_2$ occur in a document $d \\in \\mathcal{D}$ with a shared noun dependency. \n",
      "\n",
      "Let all verbs correspond to rows in a matrix $\\mathbf{W} \\in \\mathbb{R}^{V \\times \\rho}$, where $V$ is the vocabulary size and $\\rho$ is the dimensionality of our embeddings.\n",
      "\n",
      "For $n$ iterations, do:\n",
      "<ul style=\"list-style: none;\">\n",
      "    <li> For $v_1, v_2 \\in \\mathcal{C}$:</li>\n",
      "    <ul style=\"list-style: none;\">\n",
      "        <li> Sample $v_\\neg \\sim \\operatorname{NegativeSample}(\\mathcal{C}, v_1)$ until:\n",
      "        <ul style=\"list-style: none;\">\n",
      "            <li>$\\operatorname{HingeLoss}(v_1,v_2,v_\\neg) = \\max(0,1 - v_1^Tv_2 + v_1^Tv_\\neg) > 0$</li>\n",
      "            </ul></li>\n",
      "        <li>$v_1^\\prime, v_2^\\prime, v_\\neg^\\prime = \\arg\\min_\\mathbf{W} \\operatorname{HingeLoss}(v_1,v_2,v_\\neg)  $ </li>\n",
      "        <li>Project updates onto $L_2$ norm. </li>\n",
      "    </ul></ul>\n",
      "\n",
      "Updates are standard gradient descent updates: $v^{(k+1)} = v^k - \\gamma \\nabla\\operatorname{HingeLoss}(t^k,u^k,v^k)$.\n",
      "\n"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Gradients:\n",
      "\n",
      "$\\frac{\\partial}{\\partial v_1}\\operatorname{HingeLoss}(v_1,v_2,v_\\neg) = -v_2+v_\\neg$\n",
      "(this has the effect of moving $v_1$ toward $v_2$ and away from $v_\\neg$)\n",
      "\n",
      "$\\frac{\\partial}{\\partial v_2}\\operatorname{HingeLoss}(v_1,v_2,v_\\neg) = -v_1$\n",
      "\n",
      "$\\frac{\\partial}{\\partial v_\\neg}\\operatorname{HingeLoss}(v_1,v_2,v_\\neg) = v_1$\n"
     ]
    },
    {
     "cell_type": "heading",
     "level": 6,
     "metadata": {},
     "source": [
      "Criticism"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "The above model will most likely result in something similar to topic-modeling for verbs which is not the most interesting thing. Considering an extension:"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Let $\\mathcal{D}$ be a set of documents.\n",
      "\n",
      "Let $\\mathcal{C}$ be a set of tuples $(v_1,v_2, p)\\in \\mathcal{C}$ i.f.f. $v_1,v_2$ occur in a document $d \\in \\mathcal{D}$ with a shared noun dependency $p$. \n",
      "\n",
      "Let all verbs and nouns correspond to rows in a matrix $\\mathbf{W} \\in \\mathbb{R}^{V \\times \\rho}$, where $V$ is the vocabulary size and $\\rho$ is the dimensionality of our embeddings.\n",
      "\n",
      "For $n$ iterations, do:\n",
      "<ul style=\"list-style: none;\">\n",
      "    <li> For $v_1, v_2, p \\in \\mathcal{C}$:</li>\n",
      "    <ul style=\"list-style: none;\">\n",
      "        <li> Sample $v_\\neg, p_\\neg \\sim \\operatorname{NegativeSample}(\\mathcal{C}, v_1, p)$ until:\n",
      "        <ul style=\"list-style: none;\">\n",
      "            <li>$\\operatorname{HingeLoss}(v_1,v_2,p,v_\\neg,p_\\neg) = \\max(0,1 - (p+v_1)^T(p+v_2) + (p+v_1)^T(p_\\neg+ v_\\neg) > 0$</li>\n",
      "            </ul></li>\n",
      "        <li>$v_1^\\prime, v_2^\\prime, p^\\prime, v_\\neg^\\prime, p_\\neg^\\prime = \\arg\\min_\\mathbf{W} \\operatorname{HingeLoss}(v_1,v_2,p, v_\\neg, p_\\neg)  $ </li>\n",
      "        <li>Project updates onto $L_2$ norm. </li>\n",
      "    </ul></ul>\n",
      "\n",
      "Updates are standard gradient descent updates: $v^{(k+1)} = v^k - \\gamma \\nabla\\operatorname{HingeLoss}(t^k,u^k,v^k)$."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Gradients:\n",
      "\n",
      "$\\frac{\\partial}{\\partial v_1}\\operatorname{HingeLoss}(v_1,v_2,p,v_\\neg,p_\\neg) = -p-v_2+p_\\neg + v_\\neg$\n",
      "(this has the effect of moving $v_1$ toward $v_2$ and away from $v_\\neg$)\n",
      "\n",
      "$\\frac{\\partial}{\\partial v_2}\\operatorname{HingeLoss}(v_1,v_2,p,v_\\neg,p_\\neg) = -p-v_1$\n",
      "\n",
      "$\\frac{\\partial}{\\partial v_\\neg}\\operatorname{HingeLoss}(v_1,v_2,p,v_\\neg,p_\\neg) = p+v_1$\n",
      "\n",
      "$\\frac{\\partial}{\\partial p_\\neg}\\operatorname{HingeLoss}(v_1,v_2,p,v_\\neg,p_\\neg) = p+v_1$\n",
      "\n",
      "$\\frac{\\partial}{\\partial p}\\operatorname{HingeLoss}(v_1,v_2,p,v_\\neg,p_\\neg) = -p-v_1-v_2 + p_\\neg + v_\\neg$"
     ]
    },
    {
     "cell_type": "heading",
     "level": 6,
     "metadata": {},
     "source": [
      "Criticism"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "I don't think the idea above accomplishes any different? Perhaps this:"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Let $\\mathcal{D}$ be a set of documents.\n",
      "\n",
      "Let $\\mathcal{C}$ be a set of tuples $(v_1,v_2, p)\\in \\mathcal{C}$ i.f.f. $v_1,v_2$ occur in a document $d \\in \\mathcal{D}$ with a shared noun dependency $p$. \n",
      "\n",
      "Let all verbs and nouns correspond to rows in a matrix $\\mathbf{W} \\in \\mathbb{R}^{V \\times \\rho}$, where $V$ is the vocabulary size and $\\rho$ is the dimensionality of our embeddings.\n",
      "\n",
      "For $n$ iterations, do:\n",
      "<ul style=\"list-style: none;\">\n",
      "    <li> For $v_1, v_2, p \\in \\mathcal{C}$:</li>\n",
      "    <ul style=\"list-style: none;\">\n",
      "        <li> Sample $v_\\neg \\sim \\operatorname{NegativeSample}(\\mathcal{C}, v_1, p)$ until:\n",
      "        <ul style=\"list-style: none;\">\n",
      "            <li>$\\operatorname{HingeLoss}(v_1,v_2,p,v_\\neg,p_\\neg) = \\max(0,1 - p^T(v_1+v_2) + p^T(v_1+ v_\\neg) > 0$</li>\n",
      "            </ul></li>\n",
      "        <li>$v_1^\\prime, v_2^\\prime, p^\\prime, v_\\neg^\\prime = \\arg\\min_\\mathbf{W} \\operatorname{HingeLoss}(v_1,v_2,p, v_\\neg)  $ </li>\n",
      "        <li>Project updates onto $L_2$ norm. </li>\n",
      "    </ul></ul>\n"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Gradients:\n",
      "\n",
      "$\\frac{\\partial}{\\partial v_1}\\operatorname{HingeLoss}(v_1,v_2,p,v_\\neg) = -2p$\n",
      "\n",
      "$\\frac{\\partial}{\\partial v_2}\\operatorname{HingeLoss}(v_1,v_2,p,v_\\neg) = -p$\n",
      "\n",
      "$\\frac{\\partial}{\\partial v_\\neg}\\operatorname{HingeLoss}(v_1,v_2,p,v_\\neg) = p$\n",
      "\n",
      "$\\frac{\\partial}{\\partial p}\\operatorname{HingeLoss}(v_1,v_2,p,v_\\neg) = -v_1 -v_2 + v_1 + v_\\neg = -v_2 + v_\\neg$"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}